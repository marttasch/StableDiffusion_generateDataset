{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "from torch._C import TensorType\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from uuid import uuid4 as uu\n",
    "import shutil\n",
    "import os\n",
    "from tensorboard import program\n",
    "import subprocess\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# plot tensorboard metrics\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "\n",
    "\n",
    "class TensorBoard:\n",
    "    def __init__(self, tensor_board_root: str):\n",
    "        self.tensor_board_root = tensor_board_root\n",
    "        print('init writer')\n",
    "        self.writer = SummaryWriter(tensor_board_root)\n",
    "\n",
    "    def init_tensorboard(self):\n",
    "        self.gen_session_id()\n",
    "        self.launch_tensorboard()\n",
    "        self.create_launchTensorboard_script()\n",
    "\n",
    "    def launch_tensorboard(self):\n",
    "        print('launch tensorboard')\n",
    "      \n",
    "        tensorboard_command = [\"tensorboard\", \"--logdir\", self.tensor_board_root]\n",
    "        cmd = f\"start cmd /k {' '.join(tensorboard_command)}\"\n",
    "        print(f\"\\nExecuting command: {cmd}\")\n",
    "        os.system(cmd)\n",
    "\n",
    "    def create_launchTensorboard_script(self):\n",
    "        # Create a script to launch tensorboard, place it in the tensorboard root\n",
    "        script = f\"tensorboard --logdir {self.tensor_board_root}\"\n",
    "        with open(os.path.join(self.tensor_board_root, 'launch_tensorboard.bat'), 'w') as f:\n",
    "            f.write(script)\n",
    "\n",
    "    def gen_session_id(self) -> str:\n",
    "        day = str(datetime.date.today().day).rjust(2, '0')\n",
    "        month = str(datetime.date.today().month).rjust(2, '0')\n",
    "        year = str(datetime.date.today().year)[-2:]\n",
    "        hour = str(datetime.datetime.now().hour).rjust(2, '0')\n",
    "        minute = str(datetime.datetime.now().minute).rjust(2, '0')\n",
    "        self.session_id = f'{day}-{month}-{year}_{hour}-{minute}_{str(uu())[-8:]}'\n",
    "        print(f\"Session ID: {self.session_id}\")\n",
    "        return self.session_id\n",
    "\n",
    "\n",
    "    def write_board(self, epoch: int, train_loss: int, train_accuracy, train_recall, train_precision, test_loss, test_accuracy, test_recall, test_precision, current_learning_rate: float):\n",
    "      \n",
    "        # Calc metric and add scalars\n",
    "        self.writer.add_scalars(f'Loss/{self.session_id}', {'Train': train_loss, 'Test': test_loss}, epoch)\n",
    "\n",
    "        self.writer.add_scalars(f'Accuracy/{self.session_id}', {'Train': train_accuracy, 'Test': test_accuracy}, epoch)\n",
    "\n",
    "        self.writer.add_scalars(f'Recall/{self.session_id}', {'Train': train_recall, 'Test': test_recall}, epoch)\n",
    "\n",
    "        self.writer.add_scalars(f'Precision/{self.session_id}', {'Train': train_precision, 'Test': test_precision}, epoch)\n",
    "\n",
    "        self.writer.add_scalar(f'Learning Rate/{self.session_id}', current_learning_rate, epoch)\n",
    "\n",
    "        self.writer.flush()\n",
    "\n",
    "    def write_confusion_matrix(self, epoch: int, confusion_matrix):\n",
    "        # log confusion matrix\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        self.writer.add_figure('Confusion Matrix', plt.gcf(), global_step=epoch)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def write_parameter_histogram(self, model, epoch):\n",
    "        # log parameter distributions\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            plt.hist(param.flatten().detach().cpu().numpy(), bins=50, alpha=0.5, label=name)\n",
    "            plt.legend()\n",
    "            plt.title('Parameter Histograms')\n",
    "            self.writer.add_figure('Parameter Histograms', plt.gcf(), global_step=epoch)\n",
    "            self.writer.flush()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===== Plot TensorBoard Metrics =====\n",
    "\n",
    "\n",
    "    def extract_tensorboard_data(self, log_dir=None):\n",
    "        \"\"\"\n",
    "        Extracts data from TensorBoard log files.\n",
    "\n",
    "        Args:\n",
    "        - log_dir (str): Path to the directory containing TensorBoard event files.\n",
    "\n",
    "        Returns:\n",
    "        - data (dict): Dictionary containing metric data {metric_name: [(step1, value1), (step2, value2), ...]}.\n",
    "        \"\"\"\n",
    "        if log_dir is None:\n",
    "            log_dir = self.tensor_board_root\n",
    "        data = {}\n",
    "        events = {}\n",
    "\n",
    "        for root, _, files in os.walk(log_dir):\n",
    "            for file in files:\n",
    "                if file.startswith(\"events.out\"):\n",
    "                    event_path = os.path.join(root, file)\n",
    "                    for event in summary_iterator(event_path):\n",
    "                        events[event.step] = event\n",
    "                        for value in event.summary.value:\n",
    "                            if value.tag not in data:\n",
    "                                data[value.tag] = []\n",
    "                            data[value.tag].append((event.step, value.simple_value))\n",
    "        return data, events\n",
    "\n",
    "    def plot_metrics(self, metric_values, label, title, save_path):\n",
    "        \"\"\"\n",
    "        Plots and saves metrics as images.\n",
    "\n",
    "        Args:\n",
    "        - metrics (dict): Dictionary of metrics to plot.\n",
    "        - save_path (str): Path to save the plot as an image.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        steps, values = zip(*metric_values)\n",
    "        plt.plot(steps, values, label=label, marker='o', markersize=3)\n",
    "        plt.legend()\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Value')\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "        #plt.savefig(f\"{save_path}.png\")\n",
    "        plt.show()\n",
    "        #plt.close()\n",
    "\n",
    "\n",
    "    def plot_tensorboard_metrics(self, log_dir=None, title='TensorBoard Metrics', xlabel='Step', ylabel='Value', save_path='tensorboard_plot'):\n",
    "        \"\"\"\n",
    "        Extracts and plots TensorBoard metrics as images.\n",
    "\n",
    "        Args:\n",
    "        - log_dir (str): Path to the directory containing TensorBoard event files.\n",
    "        - title (str): Title of the plot.\n",
    "        - xlabel (str): Label for the x-axis.\n",
    "        - ylabel (str): Label for the y-axis.\n",
    "        - save_path (str): Path to save the plot as an image.\n",
    "        \"\"\"\n",
    "        if log_dir is None:\n",
    "            log_dir = self.tensor_board_root\n",
    "        metric_data = self.extract_tensorboard_data(log_dir)\n",
    "\n",
    "        # Split into train and test metrics\n",
    "        accuracy_values = [(step, value) for sublist in {metric: metric_data[metric] for metric in metric_data.keys() if 'Accuracy' in metric}.values() for step, value in sublist]\n",
    "        loss_values = [(step, value) for sublist in {metric: metric_data[metric] for metric in metric_data.keys() if 'Loss' in metric}.values() for step, value in sublist]\n",
    "        recall_values = [(step, value) for sublist in {metric: metric_data[metric] for metric in metric_data.keys() if 'Recall' in metric}.values() for step, value in sublist]\n",
    "        precision_values = [(step, value) for sublist in {metric: metric_data[metric] for metric in metric_data.keys() if 'Precision' in metric}.values() for step, value in sublist]\n",
    "        learning_rate_values = [(step, value) for sublist in {metric: metric_data[metric] for metric in metric_data.keys() if 'Learning Rate' in metric}.values() for step, value in sublist]\n",
    "        \n",
    "        self.plot_metrics(accuracy_values, 'Accuracy', title, save_path)\n",
    "        # Call the function for each metric\n",
    "        # self.plot_metrics(accuracies, 'accuracies_plot')\n",
    "        # self.plot_metrics(losses, 'losses_plot')\n",
    "        # self.plot_metrics(recalls, 'recalls_plot')\n",
    "        # self.plot_metrics(precisions, 'precisions_plot')\n",
    "        # self.plot_metrics(learning_rates, 'learning_rates_plot')\n",
    "        \n",
    "        return metric_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_values = [(step, value) for sublist in {metric: metric_data[metric] for metric in metric_data.keys() if 'Accuracy' in metric}.values() for step, value in sublist]\n",
    "print(f'Acc Values: {accuracy_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"D:\\\\code\\\\Bachelorarbeit\\\\xx_code_BA\\\\trainingOutput\\\\2024-05-04_19-56-23_urinal_v2_resnet50\\\\tensorboard\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init writer\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type Event is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(metric_data, f)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevents.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# metric_data = tbh.plot_tensorboard_metrics()\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# # dumb metric data as json as file\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# import json\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# with open('metric_data.json', 'w') as f:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#     json.dump(metric_data, f)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Martin\\miniconda3\\Lib\\json\\__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[0;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[0;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[0;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Martin\\miniconda3\\Lib\\json\\encoder.py:432\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Martin\\miniconda3\\Lib\\json\\encoder.py:406\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 406\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Martin\\miniconda3\\Lib\\json\\encoder.py:439\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    438\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[1;32m--> 439\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Martin\\miniconda3\\Lib\\json\\encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[0;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type Event is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import src.trainCNN.tensorboardHandler as tbh\n",
    "import json\n",
    "\n",
    "\n",
    "log_dir = \"D:\\\\code\\\\Bachelorarbeit\\\\xx_code_BA\\\\trainingOutput\\\\2024-05-04_19-56-23_urinal_v2_resnet50\\\\tensorboard\"\n",
    "\n",
    "\n",
    "tbh = TensorBoard(log_dir)\n",
    "\n",
    "\n",
    "metric_data, events = tbh.extract_tensorboard_data(log_dir)\n",
    "\n",
    "with open('metric_data.json', 'w') as f:\n",
    "    json.dump(metric_data, f)\n",
    "with open('events.json', 'w') as f:\n",
    "    json.dump(events, f)\n",
    "\n",
    "# metric_data = tbh.plot_tensorboard_metrics()\n",
    "\n",
    "# # dumb metric data as json as file\n",
    "# import json\n",
    "# with open('metric_data.json', 'w') as f:\n",
    "#     json.dump(metric_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted metrics: dict_keys(['Learning Rate', 'Accuracy_Test', 'Accuracy_Train', 'Loss_Test', 'Loss_Train', 'Precision_Test', 'Precision_Train', 'Recall_Test', 'Recall_Train'])\n",
      "\n",
      "Extracted images: dict_keys(['Confusion Matrix'])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "\n",
    "\n",
    "def load_all_events(logdir):\n",
    "    accumulators = []\n",
    "    for root, dirs, files in os.walk(logdir):\n",
    "        for file in files:\n",
    "            if file.startswith('events.out.tfevents'):\n",
    "                path = os.path.join(root, file)\n",
    "                type = root.split('_')[-1]\n",
    "                ea = event_accumulator.EventAccumulator(path,\n",
    "                    size_guidance={\n",
    "                        event_accumulator.SCALARS: 0,\n",
    "                        event_accumulator.IMAGES: 0,\n",
    "                        event_accumulator.HISTOGRAMS: 0\n",
    "                    }\n",
    "                )\n",
    "                ea.Reload()\n",
    "                accumulator = {\n",
    "                    'ea': ea,\n",
    "                    'path': path,\n",
    "                    'type': type\n",
    "                }\n",
    "                accumulators.append(accumulator)\n",
    "    return accumulators\n",
    "\n",
    "\n",
    "def read_and_plot_tensorboard_data(logdir):\n",
    "    filter_images = True\n",
    "    image_tag_whitelist = ['confusion matrix']\n",
    "\n",
    "    # Prepare directories for plots and images\n",
    "    plot_dir = os.path.join(logdir, 'plots')\n",
    "    image_dir = os.path.join(logdir, 'images')\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "    # Define the regex pattern\n",
    "    #pattern = r\"^(\\w+)_(\\d{2}-\\d{2}-\\d{2})_(\\d{2}-\\d{2})_([a-f0-9]+)_(Train|Test)$\"\n",
    "    pattern = r\"^(.+)/(\\d{2}-\\d{2}-\\d{2})_(\\d{2}-\\d{2})_([a-f0-9]+)$\"\n",
    "\n",
    "\n",
    "    # Initialize the Event Accumulator with only scalars and images\n",
    "    accumulators = load_all_events(logdir)\n",
    "\n",
    "    # Extract scalar data\n",
    "    metrics = {}\n",
    "    for accumulator in accumulators:\n",
    "        ea = accumulator['ea']\n",
    "        data_type = accumulator['type']\n",
    "        path = accumulator['path']\n",
    "\n",
    "        scalar_tags = ea.Tags()['scalars']\n",
    "        for tag in scalar_tags:\n",
    "            match = re.match(pattern, tag)\n",
    "            #print(f\"\\nTag: {tag}\")\n",
    "            if match:\n",
    "                metric_name, _, _, _ = match.groups()\n",
    "                #print(f\"Metric Name: {metric_name}, Type: {type}\")\n",
    "                if metric_name.lower() in ['accuracy', 'loss', 'recall', 'precision']:\n",
    "                    events = ea.Scalars(tag)\n",
    "                    values = [e.value for e in events]\n",
    "                    steps = [e.step for e in events]\n",
    "                    full_tag = f'{metric_name}_{data_type}'\n",
    "                    #full_tag = f'{metric_name}'\n",
    "                    metrics[full_tag] = pd.DataFrame({'Step': steps, 'Value': values})\n",
    "                   # print(f\"Values: {metrics[full_tag]}\")\n",
    "                elif metric_name.lower() in ['learning rate']:\n",
    "                    events = ea.Scalars(tag)\n",
    "                    values = [e.value for e in events]\n",
    "                    steps = [e.step for e in events]\n",
    "                    full_tag = f'{metric_name}'\n",
    "                    metrics[full_tag] = pd.DataFrame({'Step': steps, 'Value': values})\n",
    "                \n",
    "    print(f\"\\nExtracted metrics: {metrics.keys()}\")\n",
    "\n",
    "    # Plot metrics\n",
    "    for metric in ['Accuracy', 'Loss', 'Recall', 'Precision']:\n",
    "        train_tag = f'{metric}_Train'\n",
    "        test_tag = f'{metric}_Test'\n",
    "        \n",
    "        if train_tag in metrics and test_tag in metrics:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(metrics[train_tag]['Step'], metrics[train_tag]['Value'], label='Train')\n",
    "            plt.plot(metrics[test_tag]['Step'], metrics[test_tag]['Value'], label='Test')\n",
    "            plt.title(f'{metric} over Training Steps')\n",
    "            plt.xlabel('Training Steps')\n",
    "            plt.ylabel(metric)\n",
    "            plt.grid(True)\n",
    "            # datapoints with label\n",
    "            for i, txt in enumerate(metrics[train_tag]['Value']):\n",
    "                plt.annotate(f\"{txt:.2f}\", (metrics[train_tag]['Step'][i], metrics[train_tag]['Value'][i]), color='gray')\n",
    "            for i, txt in enumerate(metrics[test_tag]['Value']):\n",
    "                plt.annotate(f\"{txt:.2f}\", (metrics[test_tag]['Step'][i], metrics[test_tag]['Value'][i]), color='gray')\n",
    "\n",
    "            plt.legend()\n",
    "            plt.savefig(os.path.join(plot_dir, f'{metric}_plot.png'))\n",
    "            plt.close()\n",
    "    \n",
    "    # extract images\n",
    "    images = {}\n",
    "    for accumulator in accumulators:\n",
    "        ea = accumulator['ea']\n",
    "        path = accumulator['path']\n",
    "        image_tags = ea.Tags()['images']\n",
    "        for tag in image_tags:\n",
    "            if filter_images:\n",
    "                if tag.lower() in image_tag_whitelist:\n",
    "                    images[tag] = ea.Images(tag)\n",
    "            else:\n",
    "                images[tag] = ea.Images(tag)\n",
    "\n",
    "        \n",
    "    print(f\"\\nExtracted images: {images.keys()}\")\n",
    "\n",
    "\n",
    "    # Export images\n",
    "    for tag, image in images.items():\n",
    "        for i, img in enumerate(image):\n",
    "            img_data = io.BytesIO(img.encoded_image_string)\n",
    "            img = plt.imread(img_data)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title(tag)\n",
    "            plt.savefig(os.path.join(image_dir, f'{tag}_{i}.png'))\n",
    "            plt.close()\n",
    "\n",
    "# Example usage:\n",
    "# read_and_plot_tensorboard_data('./path/to/logdir')\n",
    "\n",
    "log_dir = \"D:\\\\code\\\\Bachelorarbeit\\\\xx_code_BA\\\\trainingOutput\\\\2024-05-04_21-20-13_urinal_v2_resnet50\\\\tensorboard\"\n",
    "read_and_plot_tensorboard_data(log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.annotate(f\"{txt:.2f}\", (metrics[train_tag]['Step'][i], metrics[train_tag]['Value'][i]), color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Name: Learning Rate\n",
      "Date: 04-05-24\n",
      "Time: 21-20\n",
      "ID: f133eb55\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tag = \"Learning Rate/04-05-24_21-20_f133eb55\"\n",
    "pattern = r\"^(.+)/(\\d{2}-\\d{2}-\\d{2})_(\\d{2}-\\d{2})_([a-f0-9]+)$\"\n",
    "match = re.match(pattern, tag)\n",
    "\n",
    "if match:\n",
    "    metric_name, date, time, id = match.groups()\n",
    "    print(f\"Metric Name: {metric_name}\")\n",
    "    print(f\"Date: {date}\")\n",
    "    print(f\"Time: {time}\")\n",
    "    print(f\"ID: {id}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Metric': 'Accuracy', 'Date': '04-05-24', 'Time': '14-43', 'Hash': 'c6332f5a', 'Type': 'Test'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_tag(tag):\n",
    "    pattern = r\"^(\\w+)_(\\d{2}-\\d{2}-\\d{2})_(\\d{2}-\\d{2})_([a-f0-9]+)_(Train|Test)$\"\n",
    "    match = re.match(pattern, tag)\n",
    "    if match:\n",
    "        return {\n",
    "            'Metric': match.group(1),\n",
    "            'Date': match.group(2),\n",
    "            'Time': match.group(3),\n",
    "            'Hash': match.group(4),\n",
    "            'Type': match.group(5)\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example\n",
    "tag = \"Accuracy_04-05-24_14-43_c6332f5a_Test\"\n",
    "parsed_info = parse_tag(tag)\n",
    "print(parsed_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to datetime.timedelta.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m endtime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m      7\u001b[0m timeelapsed \u001b[38;5;241m=\u001b[39m endtime \u001b[38;5;241m-\u001b[39m starttime\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtimeelapsed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to datetime.timedelta.__format__"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "time.sleep(2)\n",
    "endtime = datetime.datetime.now()\n",
    "timeelapsed = endtime - starttime\n",
    "timeelapsed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending Gotify message: CNN Training interrupted with error - Error: Test Error \n",
      " Time elapsed: 1.0006401538848877\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import gotifyHandler as gotifyHandler\n",
    "import time\n",
    "\n",
    "def get_TimeElapsed(starttime):\n",
    "    endtime = time.time()\n",
    "    timeelapsed = endtime - starttime\n",
    "    string = str(timeelapsed)\n",
    "    return string\n",
    "\n",
    "e = 'Test Error'\n",
    "starttime = time.time()\n",
    "\n",
    "# wait 1 second\n",
    "time.sleep(1)\n",
    "\n",
    "gotifyHandler.send_message(\n",
    "    title='CNN Training interrupted with error',\n",
    "    message=f\"\"\"Error: {e} \\n Time elapsed: {get_TimeElapsed(starttime)}\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Martin\\AppData\\Local\\Temp\\ipykernel_41504\\1506348261.py\", line 5, in <module>\n",
      "    print(1/0)\n",
      "          ~^~\n",
      "ZeroDivisionError: division by zero\n"
     ]
    }
   ],
   "source": [
    "# test try except error handling with traceback\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    print(1/0)\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    with open('error.txt', 'w') as f:\n",
    "        traceback.print_exc(file=f)\n",
    "        f.write('\\n\\n')\n",
    "        traceback.print_stack(file=f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:02.00\n"
     ]
    }
   ],
   "source": [
    "from src.mts_utils import *\n",
    "\n",
    "end_time = time.time()  # Record end time of epoch\n",
    "start_time = time.time()  # Record start time of epoch\n",
    "\n",
    "time.sleep(2)\n",
    "epoch_time = get_TimeElapsed(start_time)\n",
    "\n",
    "\n",
    "print(epoch_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 1717242710.3453424\n",
      "00:00:02.00\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def get_TimeStamp(time):\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "start_time = time.time()\n",
    "print(f\"Start Time: {start_time}\")\n",
    "time.sleep(2)\n",
    "print(get_TimeElapsed(start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'strftime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtime_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m, in \u001b[0;36mtime_to_string\u001b[1;34m(time)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtime_to_string\u001b[39m(time):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'strftime'"
     ]
    }
   ],
   "source": [
    "def time_to_string(time):\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "start_time = time.time()\n",
    "print(f\"Start Time: {time_to_string(start_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+-------+--------+----------+--------+-----------+\n",
      "|       |  Loss  | Accuracy | Recall | Precision |\n",
      "+-------+--------+----------+--------+-----------+\n",
      "| Train | 0.5678 |  0.5678  | 0.9012 |   0.3456  |\n",
      "|  Test | 0.1234 |  0.5678  | 0.9012 |   0.3456  |\n",
      "+-------+--------+----------+--------+-----------+\n",
      "Train Accuracy: 0.5678\n",
      "0.567799985408783\n",
      "tensor(0.5678)\n"
     ]
    }
   ],
   "source": [
    "import prettytable as pt\n",
    "import torch\n",
    "\n",
    "\n",
    "# dummy values as tensors\n",
    "train_loss = torch.tensor(0.1234)\n",
    "train_acc = torch.tensor(0.5678)\n",
    "train_rec = torch.tensor(0.9012)\n",
    "train_pre = torch.tensor(0.3456)\n",
    "test_loss = torch.tensor(0.1234)\n",
    "test_acc = torch.tensor(0.5678)\n",
    "test_rec = torch.tensor(0.9012)\n",
    "test_pre = torch.tensor(0.3456)\n",
    "epoch_loss_test = torch.tensor(0.1234)\n",
    "epoch_loss_train = torch.tensor(0.5678)\n",
    "\n",
    "table = pt.PrettyTable()\n",
    "table.field_names = [\"\", \"Loss\", \"Accuracy\", \"Recall\", \"Precision\"]\n",
    "table.add_row([\"Train\", f\"{epoch_loss_train:.4f}\", f\"{train_acc:.4f}\", f\"{train_rec:.4f}\", f\"{train_pre:.4f}\"])\n",
    "table.add_row([\"Test\", f\"{epoch_loss_test:.4f}\", f\"{test_acc:.4f}\", f\"{test_rec:.4f}\", f\"{test_pre:.4f}\"])\n",
    "print(f\"\\n{table}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(train_acc.item())\n",
    "print(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
